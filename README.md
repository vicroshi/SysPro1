Λίγα λόγια για το client/server μοντέλο και το πρωτόκολλο επικοινωνίας:
Αρχικά ο jobCommander κάνει το parsing των ορισμάτων που δίνονται από τον χρήστη
και ελέγχει αν είναι σωστά. Στη συνέχεια ελέγχει αν ο server τρέχει και αν όχι
τον γεννάει. Χρησιμοποίησα μια παραλλαγή της τεχνική του double fork ή 
daemonisation για τον σέρβερ, οπότε αφού γίνει fork/exec'd ο server, ο 
commander 
θα τον περιμένει με waitpid. Από την μεριά του server, το original process θα 
έχει ετοιμάσει τα pipes, τα fds και το αρχείο .txt μετά θα κάνει fork/exec τον 
worker. Ο worker θα συνεχίσει κάνοντας την κανονική λειτουργία του server 
και το original process θα τερματίσει. Έτσι ο commander θα μπορέσει να συνεχίσει
και να ανοίξει με τη σειρά του τα pipes που χρειάζεται. Το πρωτόκολλο που 
επέλεξα είναι σχετικά απλό. Κάθε client(commander) γράφει στο pipe του server 
που είναι πάντα ανοιχτό, requests της μορφής struct request_header αυτό 
είναι εξασφαλισμένο ότι θα έχει atomic write άρα δεν υπάρχει φόβος να 
μπερδευτούν τα requests. Ο server διαβάζει από το pipe του και ανάλογα με το 
command_num πάει στο αντίστοιχο case. Αν το command_num είναι 1 δηλαδή ο client
θέλει να κάνει issueJob, τότε θα πρέπει να το γράψει σε ξεχωριστό pipe για 
να εξασφαλιστεί το atomicity. Ο server θα δει το command_num και θα πάει να 
ανοίξει αυτό το pipe και θα διαβάσει το message. Κάθε client επίσης, έχει 
ένα δικό του pipe για να λαμβάνει τα αποτελέσματα των requests του, κάθε 
απάντηση που λαμβάνει έχει οπωσδήποτε ενά struct response_header και μπορεί 
να έχει και ένα message/payload, ανάλογα με τις τιμές στο response_header, 
καταλαβαίνει ο client, αν πρέπει να υπάρχει το message. Λεπτομέρειες για τα 
structs και το πρωτόκολλο δίνονται σε σχόλια στον κώδικα.
Λίγα λόγια για τις δομές που χρησιμοποιεί ο server:
Ο server χρησιμοποιεί τη δομή struct job που περιγράφει ένα job και είναι το 
datatype των στοιχείων της ουράς. Η ουρά είναι ουσιαστικά ένα intrusive list,  
δηλαδή η είναι άμεσα συνδεδεμένη με τα δεδομένα/αντικείμενα μας, αυτό 
φαίνεται από το γεγονός πως κάθε struct job έχει ενα struct QueueNode. Τέτοιου
είδους δομές χρησιμοποιούν τις δομές που σκοπεύουν να αποθηκεύσουν ως μέσα 
αποθήκευσης. Η δομή struct Queue είναι απλός δυο δέικτες στα nodes της ουράς, 
ένας στην αρχή και ένας στο τέλος και το size της. Εισαγωγές γίνονται με τις 
enqueue/dequeue και removeNodeId. Το queuePosition αν και υπάρχει και 
ενημερώνομαι για τυπικούς λόγους, κανονικά δεν είναι απαραίτητο αφού είναι το 
index της ουράς και προσαρμόζεται απο μόνο του αφού πρόκειται για συνδεδεμένη 
λίστα. Επέλεξα να έχω intrusive data structure γιατί είχα διαβάσει οτι ο linux 
kernel χρησιμοποιεί για τα task queues του και θεώρησα ότι ταιριάζει στην 
περίπτωση της εργασίας, επίσης κάνει λιγότερα allocation calls καθώς χρειάζεται 
μόνο για κάθε job και όχι για κάθε node. Επίσης, η δομή αυτή είναι πιο αποδοτική 
σε cache misses αφού κάνεις iterate μόνο τα nodes και τα στοιχεία προσπελάζονται
με το macro getJob (ουσιαστικά μια container_of) που είναι πιο αποδοτικό απο 
dereferencing. Επίσης, μου επιτρέπει να έχω 2 ουρές χωρίς να χρειαστεί τον 
διπλάσιο αριθμό από allocations.
Λίγα λόγια για τον κώδικα του server:
Για τον server χρησιμοποίησα την poll() για να περιμένω reads στο server 
pipe που δέχεται τα requests των clients. Επίσης, χρησιμοποίησα τη signalfd()
για το SIGCHLD για να μπορώ να κάνω polling και σε αυτό. Κάθε φορά που 
λαμβάνει event απο την poll, ελέγχει αν είναι απο το fd του SIGCHLD, βγάζει
από την ενεργή ουρά (running queue) τα jobs που τερμάτισαν και βάζει αυτά από 
την ανενεργή ουρά (queue). Μετά κοιτάει για requests από τους clients και τα 
εκτελεί σειριακά μέχρι να διαβάσει όλο το buffer ώστε να ξαναπάει στο poll.
Για τα responses στα requests που είναι poll running/queued χρησιμοποίησα την
writev() γιατί θεώρησα οτί είναι κατάλληλη αφού έχω scattered buffers (τα job 
descriptions) και ήθελα να τα γράψω με ελάχιστο syscall overhead.
Παραπάνω λεπτομέρειες για τα σημεία του κώδικα βρίσκονται σε σχόλια.